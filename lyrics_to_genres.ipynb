{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics to genres multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as tfb\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r\"data_cleaned/final.csv\")\n",
    "data = pd.read_csv(r\"data_cleaned/data_final2_equalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique genre labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "for row in data.genre:\n",
    "    for item in row.split(\",\"):\n",
    "        unique_genres.add(item)\n",
    "\n",
    "unique_genres = list(unique_genres)\n",
    "print(len(unique_genres))\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_genres(genres_list):\n",
    "    new_genres = []\n",
    "    \n",
    "    for genre in genres_list.split(\",\"):\n",
    "        if genres[genre] >= 930:\n",
    "            new_genres.append(genre)\n",
    "    \n",
    "    return ','.join(new_genres) \n",
    "\n",
    "\n",
    "def get_new_genres(df):\n",
    "    genres = dict()\n",
    "    \n",
    "    for row in df.genre:\n",
    "        for item in row.split(\",\"):\n",
    "            if item in genres:\n",
    "                genres[item] += 1\n",
    "            else:\n",
    "                genres[item] = 1\n",
    "    \n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = get_new_genres(data)\n",
    "\n",
    "data[\"genre\"] = data[\"genre\"].apply(lambda x: drop_genres(x))\n",
    "data = data.drop(data[data[\"genre\"] == \"\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping songs where length of lyrics is less than 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data[\"lyrics\"].map(lambda x: len(x.split())) < 100].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of genre occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = get_new_genres(data)\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "plt.bar(genres.keys(), genres.values())\n",
    "plt.title(\"Number of genre occurances\")\n",
    "plt.ylabel('Number of occurances')\n",
    "plt.xlabel('Genre')\n",
    "plt.xticks(list(genres.keys()), rotation=75, fontsize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vectors with encoded genre labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(x):\n",
    "    labels = np.array([0 for i in range(len(unique_genres))])\n",
    "\n",
    "    for item in x.split(\",\"):\n",
    "        labels[unique_genres.index(item)] = 1\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"genre\"] = data[\"genre\"].apply(lambda x: x.replace(\"\\n\",\" \"))\n",
    "data[\"labels\"] = data[\"genre\"].apply(lambda x: set_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text): \n",
    "    text = re.sub(r'[?|!|\\'|\"|#]', r'',text)\n",
    "    text = re.sub(r'[.|,|)|(|\\|/]', r' ',text)\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = \"\"\n",
    "    \n",
    "    for word in text.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n",
    "        result += alpha_word\n",
    "        result += \" \"\n",
    "    result = result.strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def ultimate_text_cleaning(text):\n",
    "    text = clean_text(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lyrics\"] = data[\"lyrics\"].apply(lambda x: ultimate_text_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lyrics = [i for i in data[\"lyrics\"][:3]]\n",
    "print(sample_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(r\"data_cleaned/final_cleaned_labeled2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"data_cleaned/final_cleaned_labeled2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(x):\n",
    "    labels = np.array([0 for i in range(len(unique_genres))])\n",
    "\n",
    "    for item in x.split(\",\"):\n",
    "        labels[unique_genres.index(item)] = 1\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "for row in data.genre:\n",
    "    for item in row.split(\",\"):\n",
    "        unique_genres.add(item)\n",
    "\n",
    "unique_genres = list(unique_genres)\n",
    "\n",
    "data[\"genre\"] = data[\"genre\"].apply(lambda x: x.replace(\"\\n\",\" \"))\n",
    "data[\"labels\"] = data[\"genre\"].apply(lambda x: set_labels(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset to test and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data[\"lyrics\"], data[\"labels\"], test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.stack(y_train.values).astype('float32')\n",
    "y_test = np.stack(y_test.values).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check statistics for  maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_words_num(x):\n",
    "  x = x.split(\" \")\n",
    "  return len(x)\n",
    "\n",
    "print(data[\"lyrics\"].apply(calculate_words_num).mean())\n",
    "print(data[\"lyrics\"].apply(calculate_words_num).median())\n",
    "print(data[\"lyrics\"].apply(calculate_words_num).max())\n",
    "print(data[\"lyrics\"].apply(calculate_words_num).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100_000\n",
    "maxlen = 200\n",
    "output_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPool1D, MaxPooling1D, Activation, Dropout, Conv1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "for row in data.genre:\n",
    "    for item in row.split(\",\"):\n",
    "        unique_genres.add(item)\n",
    "\n",
    "num_classes = len(unique_genres)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=output_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(100, kernel_regularizer='l1_l2'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes, name=\"output\"))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss function to give more weigth for 1's (because of multilabeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_WEIGHT = 10  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor\n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier\n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.math.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(labels=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "\n",
    "get_custom_objects().update({\"weighted_binary_crossentropy\": weighted_binary_crossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_logit):\n",
    "    \"\"\"\n",
    "    Calculate F1 score\n",
    "    y_true: true value\n",
    "    y_logit: predicted value\n",
    "    \"\"\"\n",
    "    true_positives = tfb.sum(tfb.round(tfb.clip(y_true * y_logit, 0, 1)))\n",
    "    possible_positives = tfb.sum(tfb.round(tfb.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + tfb.epsilon())\n",
    "    predicted_positives = tfb.sum(tfb.round(tfb.clip(y_logit, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + tfb.epsilon())\n",
    "    return (2 * precision * recall) / (precision + recall + tfb.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=\"adam\", \n",
    "#               loss=tf.nn.sigmoid_cross_entropy_with_logits, \n",
    "#               metrics=['accuracy', f1_score, tf.keras.metrics.AUC()])\n",
    "\n",
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=['accuracy', f1_score, tf.keras.metrics.AUC()])\n",
    "\n",
    "# model.compile(optimizer=\"adam\", \n",
    "#               loss=\"weighted_binary_crossentropy\", \n",
    "#               metrics=['accuracy', f1_score, tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metrics history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'], label='loss')\n",
    "plt.plot(hist.history['val_loss'], label='val_loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy\")\n",
    "plt.plot(hist.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"F1 score\")\n",
    "plt.plot(hist.history[\"f1_score\"], label=\"f1_score\")\n",
    "plt.plot(hist.history[\"val_f1_score\"], label=\"val_f1_score\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "\n",
    "indexes_pred = np.argwhere(y_pred[k] >= 0.5)\n",
    "indexes_true = np.argwhere(y_test[k] == 1)\n",
    "\n",
    "indexes_pred.reshape((1,-1))\n",
    "indexes_true.reshape((1,-1))\n",
    "\n",
    "for i in indexes_pred:\n",
    "  print(list(unique_genres)[i[0]])\n",
    "\n",
    "print(y_pred[k])\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in indexes_true:\n",
    "  print(list(unique_genres)[i[0]])\n",
    "\n",
    "print(y_test[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'models/model_cnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(r'models/model_cnn.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=output_dim, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "                             recurrent_dropout=0.0, dropout=0.5, kernel_initializer='glorot_uniform'),\n",
    "\t                      merge_mode='concat'))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "                             recurrent_dropout=0.0, dropout=0.5, kernel_initializer='glorot_uniform'),\n",
    "\t                      merge_mode='concat'))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"weighted_binary_crossentropy\", \n",
    "              metrics=['accuracy', f1_score, tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'models/model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(r'models/model_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "\n",
    "indexes_pred = np.argwhere(y_pred[k] >= 0.9)\n",
    "indexes_true = np.argwhere(y_test[k] == 1)\n",
    "\n",
    "indexes_pred.reshape((1,-1))\n",
    "indexes_true.reshape((1,-1))\n",
    "\n",
    "for i in indexes_pred:\n",
    "  print(list(unique_genres)[i[0]])\n",
    "\n",
    "print(y_pred[k])\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in indexes_true:\n",
    "  print(list(unique_genres)[i[0]])\n",
    "\n",
    "print(y_test[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fasttext = KeyedVectors.load_word2vec_format(r'pretrained_embeddings/crawl-300d-2M-subword.vec', binary=False, encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data[\"lyrics\"], data[\"labels\"], test_size=0.2, shuffle=True)\n",
    "y_train = np.stack(y_train.values).astype('float32')\n",
    "y_test = np.stack(y_test.values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "nb_of_unknown_words = 0\n",
    "\n",
    "weight_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = fasttext[word]\n",
    "        weight_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        weight_matrix[i] = np.random.uniform(-5, 5, embedding_dim)\n",
    "        nb_of_unknown_words += 1\n",
    "        \n",
    "print(f\"Number of unknown words inizialized randomly: {nb_of_unknown_words}\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, weights=[weight_matrix], trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, kernel_regularizer='l1_l2', activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50, kernel_regularizer='l1_l2', activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"weighted_binary_crossentropy\", \n",
    "              metrics=[f1_score, tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 20\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, weights=[weight_matrix], trainable=False))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer='l1_l2'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer='l1_l2'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(num_classes, name=\"output\"))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"weighted_binary_crossentropy\", \n",
    "              metrics=['accuracy', f1_score, tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'models/model_cnn_fasttext.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(r'models/model_cnn_fasttext.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, weights=[weight_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "                             recurrent_dropout=0.0, dropout=0.5, kernel_initializer='glorot_uniform'),\n",
    "\t                    merge_mode='concat'))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "                             recurrent_dropout=0.0, dropout=0.5, kernel_initializer='glorot_uniform'),\n",
    "\t                      merge_mode='concat'))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"weighted_binary_crossentropy\", \n",
    "              metrics=['accuracy', f1_score, tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(r'models/model_lstm_fasttext2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(r'models/model_lstm_fasttext.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "\n",
    "indexes_pred = np.argwhere(y_pred[k] >= 0.7)\n",
    "indexes_true = np.argwhere(y_test[k] == 1)\n",
    "\n",
    "indexes_pred.reshape((1,-1))\n",
    "indexes_true.reshape((1,-1))\n",
    "\n",
    "for i in indexes_pred:\n",
    "  print(list(unique_genres)[i[0]])\n",
    "\n",
    "print(y_pred[k])\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in indexes_true:\n",
    "  print(list(unique_genres)[i[0]])\n",
    "\n",
    "print(y_test[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(model, threshold=0.75):\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    y_pred[y_pred > threshold] = 1\n",
    "    y_pred[y_pred <= threshold] = 0\n",
    "\n",
    "    return multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "def calculate_confusion_ratio(matrix):\n",
    "    ok = sum([matrix[i, 0, 0] + matrix[i, 1, 1] for i in range(matrix.shape[0])])\n",
    "    bad = sum([matrix[i, 1, 0] + matrix[i, 0, 1] for i in range(matrix.shape[0])])\n",
    "    \n",
    "    print(f\"percent of ok: {ok/(ok+bad)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_confusion_matrix(model, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confusion_ratio(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83fd4375a822156133a8f0ea701c86b5bfda9cc7c150a00174c57fbf773ec247"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
